---
abstractspacing: double
appendix: false
fontfamily: libertine
monofont: inconsolata
monofontoptions: scaled=.95
fontsize: 12pt
geometry: 
  - top=0.5cm
  - bottom=0.5cm
  - left=1.5cm
  - right=1.5cm
urlcolor: darkblue
highlight-style: arrow
csl: /home/mathias/Descargas/apa.csl
bibliography: /home/mathias/Descargas/Kinetoplastidos.bib
reference-section-title: "Referencias"
lang: es

format: 
    pdf:
      toc: false
      toc-depth: 2
      number-sections: true
      colorlinks: true
      link-citations: true
      linkcolor: black
      urlcolor: blue
      citecolor: black
      fig-cap-location: top
---

<div style="text-align: left;">
**Mathias**
</div>
<div style="text-align: right;">
26 Octubre, 2025
</div>

# Descripción del Pipeline y Principios de Ejecución mediante Snakemake

El workflow implementado se organiza mediante Snakemake, un sistema de gestión de flujos reproducibles el cual se utiliza ampliamente para los analisis bioinformáticos. Snakemake permite estructurar un conjunto de tareas dependientes entre sí, garantizando reproducibilidad, escalabilidad y actualización automática de resultados. El enfoque principal se basa en describir qué archivos deben ingresar y cuales deben obtenerse y dejar que Snakemake determine cómo generarlos, ejecutando solamente los pasos necesarios en función de las dependencias entre reglas.

En este contexto, el presente workflow se crea a partir de ciertos análisis que se desprenden de mi pryecto de maestría, los cuales son utilizados durante el proyecto para crear un sitio web. Sin enbargo, no se incluye la totalidad de los análisis originales.

Este workflow consta de 8 `rules`, una `rule` es una unidad que define un paso en el workflow. Se le debe especificar al menos un `input`, `output` y `comando` o `función` que debe ejecutar. Es similar a una función ya que encapsula y se le debe pasar ciertos argumentos, pero tiene un proposito distinto a una función. Las rules que utilizamos son:

- all
- validate_fasta
- infoseq_stat
- combine_tables
- plot_stats
- frequency_analysis
- plot_frequency
- report

## Repositorio público

El workflow está disponible en GitHub: https://github.com/mathiashole/hi-snakemake  
Clonar: git clone https://github.com/mathiashole/hi-snakemake.git
Descargar: wget https://github.com/mathiashole/hi-snakemake/archive/refs/heads/main.zip

El repositorio contiene el Snakefile (archivo de ejecucion), scripts auxiliares (carpeta scripts), config.yaml y environment.yaml. Consulte el README para instalar dependencias (seqkit, infoseq, R/paquetes, Quarto) y las instrucciones de ejecución (p. ej. `snakemake --cores N`).

## Configuración del entorno y detección automática de datos

Para el correcto funcionamiento es necesaria la instalación de `Pixi`, una herramienta de gestion de entornos. Asimismo, seguir los pasos especificos del README que se encuentra en el repositorio publico anteriormente mencionado llamado `hi-snakemake`.

Este repositorio trae consigo un archivo `environment.yaml`, este es un archivo en formato YAML usado para administrar entornos de software, más comúnmente para el administrador de paquetes de Conda. Especifica el nombre, las dependencias y los canales del entorno, permitiendo a los usuarios recrear un entorno idéntico en diferentes máquinas. En este contexto, `channels` son los sitios donde Conda descarga los paquetes que necesita para el entorno, son repositorios que contienen programas y librerias. `Dependencies` son los programas y librerias que queremos que se instalen desde estos repositorios.

El pipeline inicia declarando un archivo de configuración (config.yaml) y emplea la función **glob_wildcards()** para buscar de manera automática todos los archivos que terminan en `.fasta` y que esten presentes en el directorio `data/`. La expresion utilizada `data/{fname}.fasta` extrae la parte variable `fname` guardandola en una lista `FNAME`. Permitiendo asi, que el análisis incorpore nuevos archivos sin necesidad de modificar nada de codigo.

Por otro lado, generamos una lista completa de fastas con **expand()**, haciendo uso de la variable `{fname}` se genera la lista guardandola en la variable `FASTA`. Ademas, para poder reutilizar una misma rule se uso parámetros como el tipo de análisis de frecuencia, que se extraen directamente desde el archivo de configuración, manteniendo el pipeline flexible, reproducible y fácil de extender.

Asimismo, los parámetros definidos en el archivo de configuración denominado **config.yaml** permiten controlar aspectos del análisis (por ejemplo, el tipo de frecuencia nucleotídica a calcular) sin editar el código del pipeline.

## Definición global del workflow

La regla principal (rule `all`) especifica los archivos que representan los resultados finales del análisis (`output`), estos son tablas individuales y combinadas, gráficos guardados en formato PDF, salidas del análisis de frecuencia de dinucleótidos, trinucleótidos y un reporte final en formato HTML, asociado a documentos css, imagenes y javascript. Snakemake construye automáticamente un grafo acíclico dirigido (DAG) que describe todas las dependencias necesarias para producir dichos resultados. De esta forma, el flujo se ejecuta de manera ordenada, asegurando que cada paso se lleve a cabo únicamente cuando las entradas requeridas están disponibles.

## Validación inicial

Las primeras dos reglas del workflow (`validate_fasta` e `infoseq_stats`) operan sobre cada archivo FASTA de forma individual y paralizable. Snakemake identifica que estas tareas son independientes entre sí y las ejecuta en paralelo cuando los recursos de la maquina lo permiten. La validación mediante seqkit es (meramente ilustrativa para incorporar otros software en la ejecucion del pipeline, dado que se puede hacer con infoseq), el comando utilizado genera estadisticos generales basicos de cada archivo fasta. Luego, sigue por la rule `infoseq_stats` la cual obtiene estadísticas básicas de cada secuencia dentro de los archivos multi fasta mediante infoseq constituyen el preprocesamiento principal de los datos de entrada.

## Integración de resultados intermedios

Una vez generadas las tablas individuales, con la siguiente regla `combine_tables` se genera una tabla maestra con toda la información en un único archivo en formato tabular. Snakemake asegura que esta regla no se ejecute hasta que se hayan producido todas las tablas de entradas necesarias para generar esta tabla maestra. Asimismo, si alguna tabla cambia o se agrega un FASTA nuevo, únicamente las etapas afectadas serán reevaluadas, evitando la re-ejecución completa del workflow, de esta manera se ahorra capacidad de procesamiento.

## Análisis de frecuencias de nucleótidos

Este pipeline incluye la posibilidad de calcular frecuencias dinucleotídicas y trinucleotídicas mediante la regla "`frequency_analysis`", la cual se parametriza a partir del archivo configuración "`config.yaml`". Gracias a esto, Snakemake determina de forma automática si debe calcular dinucleótidos, trinucleótidos o ambos. La regla se expande dinámicamente para cada tipo de análisis solicitado, generando las tablas correspondientes.

En esta sección se utiliza extensivamente el mecanismo de **wildcards**, tanto para definir los archivos de salida (`output`) como para indicar qué tipo de cálculo debe realizar el script. Esto aporta modularidad a la regla y facilita incorporar nuevos tipos de análisis sin necesidad de agregar múltiples líneas adicionales de código.

Finalmente, se incluye una instrucción para mover los resultados generados al directorio de salida del pipeline, ya que el script que llama la regla, guarda los archivos en el directorio de ejecución del snakefile. Este paso asegura consistencia en la estructura de resultados. Además, evita el conflicto con los argumentos que se definieron en la regla.

## Generación de gráficos

Mediante las reglas "`plot_stats`" y "`plot_frequency`", Snakemake ejecuta scripts en R que producen diferentes visualizaciones basados en la tablas combinadas de longitud de secuencia, contenido de GC y frecuencias nucleotídicas. Estas reglas son totlamnete dependientes de las tablas generadas en etapas anteriores, por lo que Snakemake garantiza su actualización solo cuando los datos de entrada hayan cambiado.

Este comportamiento dirigido por dependencias aporta una gran eficiencia al pipeline, especialmente en análisis iterativos o cuando se trabaja con un volumen elevado de datos, ya que evita la regeneración innecesaria de gráficos.

7. Construcción del reporte final

El workflow concluye con la compilación de un reporte en formato HTML mediante Quarto. Snakemake supervisa que tanto los resultados numéricos como las figuras necesarias estén disponibles previo a la generación del reporte. Gracias al manejo automático de dependencias, cualquier modificación en los datos de entrada o en los pasos previos resulta en la actualización del documento final, asegurando consistencia entre análisis y reporte.

Conclusión

El pipeline desarrollado utiliza Snakemake como herramienta central para garantizar un análisis reproducible, modular y eficiente. Mediante la definición explícita de reglas, entradas y salidas, Snakemake automatiza la ejecución ordenada del flujo, paraleliza tareas independientes, actualiza únicamente los pasos necesarios ante cambios en los datos, y facilita la integración de múltiples herramientas externas como seqkit, infoseq, R y Quarto. El resultado es un sistema robusto, escalable y fácilmente extensible para el procesamiento automatizado de archivos FASTA y la generación de reportes analíticos.

